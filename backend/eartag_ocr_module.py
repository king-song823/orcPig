# -*- coding: utf-8 -*-
"""
çŒªè€³æ ‡è¯†åˆ«æ¨¡å— - ç‹¬ç«‹æ¨¡å—
ä¸“é—¨ç”¨äºè¯†åˆ«çŒªè€³æ ‡ä¸­çš„7ä½å’Œ8ä½æ•°å­—
åŸºäºdemo_eartag_ocr.pyçš„ç§‘å­¦æ–¹æ¡ˆ
"""

import logging
import cv2
import numpy as np
import re
from paddleocr import PaddleOCR

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class EartagOCR:
    """çŒªè€³æ ‡OCRè¯†åˆ«ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        self.ocr = PaddleOCR(
            use_angle_cls=True,      # æ–‡æœ¬æ–¹å‘åˆ†ç±»
            lang='ch',               # ä¸­æ–‡+æ•°å­—
            use_gpu=False,           # CPU æ¨¡å¼
            det_db_thresh=0.005,     # æä½æ£€æµ‹é˜ˆå€¼ï¼Œæé«˜8ä½æ•°å­—æ£€æµ‹æ•æ„Ÿåº¦
            det_db_box_thresh=0.05,  # æä½æ¡†é˜ˆå€¼
            det_db_unclip_ratio=5.0, # å¢åŠ æœªè£å‰ªæ¯”ä¾‹ï¼Œæ•è·æ›´å¤šæ•°å­—
            drop_score=0.005,        # æä½ç½®ä¿¡åº¦é˜ˆå€¼
            max_text_length=100,     # å¢åŠ æœ€å¤§æ–‡æœ¬é•¿åº¦
            det_limit_side_len=960,  # å¢åŠ æ£€æµ‹å›¾åƒå°ºå¯¸
            det_limit_type='max',    # ä½¿ç”¨æœ€å¤§è¾¹é™åˆ¶
            rec_batch_num=6,         # å¢åŠ è¯†åˆ«æ‰¹å¤„ç†æ•°é‡
            show_log=False
        )
    
    def is_valid_eartag_number(self, text):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è€³æ ‡æ•°å­—"""
        # æ¸…ç†æ–‡æœ¬ï¼Œåªä¿ç•™æ•°å­—å’Œå­—æ¯
        clean_text = ''.join(c for c in text if c.isalnum())
        
        # è€³æ ‡æ•°å­—é€šå¸¸çš„ç‰¹å¾ï¼š
        # 1. é•¿åº¦å¿…é¡»æ˜¯7ä½æˆ–8ä½
        # 2. ä¸»è¦åŒ…å«æ•°å­—
        # 3. å¯èƒ½åŒ…å«å°‘é‡å­—æ¯
        if len(clean_text) != 7 and len(clean_text) != 8:
            return False
        
        # æ•°å­—å æ¯”åº”è¯¥è¶…è¿‡70%
        digit_count = sum(1 for c in clean_text if c.isdigit())
        if digit_count / len(clean_text) < 0.7:
            return False
        
        return True
    
    def detect_and_correct_rotation(self, img):
        """æ£€æµ‹å¹¶æ ¡æ­£å›¾åƒæ—‹è½¬"""
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            if len(img.shape) == 3:
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            else:
                gray = img.copy()
            
            # è¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(gray, 50, 150, apertureSize=3)
            
            # éœå¤«ç›´çº¿æ£€æµ‹
            lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)
            
            if lines is not None and len(lines) > 0:
                # è®¡ç®—ä¸»è¦è§’åº¦
                angles = []
                for line in lines:
                    rho, theta = line[0]
                    angle = theta * 180 / np.pi
                    if angle > 90:
                        angle = angle - 180
                    angles.append(angle)
                
                # è®¡ç®—å¹³å‡è§’åº¦
                avg_angle = np.mean(angles)
                logger.info(f"ğŸ” æ£€æµ‹åˆ°æ—‹è½¬è§’åº¦: {avg_angle:.2f}åº¦")
                
                # å¦‚æœè§’åº¦å¤§äºé˜ˆå€¼ï¼Œè¿›è¡Œæ ¡æ­£
                if abs(avg_angle) > 5:
                    # è®¡ç®—æ—‹è½¬ä¸­å¿ƒ
                    height, width = gray.shape[:2]
                    center = (width // 2, height // 2)
                    
                    # åˆ›å»ºæ—‹è½¬çŸ©é˜µ
                    rotation_matrix = cv2.getRotationMatrix2D(center, -avg_angle, 1.0)
                    
                    # æ‰§è¡Œæ—‹è½¬
                    corrected = cv2.warpAffine(img, rotation_matrix, (width, height))
                    logger.info(f"âœ… å·²æ ¡æ­£æ—‹è½¬è§’åº¦: {avg_angle:.2f}åº¦")
                    return corrected
            
            return img
            
        except Exception as e:
            logger.error(f"æ—‹è½¬æ£€æµ‹é”™è¯¯: {e}")
            return img
    
    def preprocess_image_for_eartag(self, img):
        """ä¸“é—¨é’ˆå¯¹çŒªè€³æ ‡çš„å›¾åƒé¢„å¤„ç† - å¢å¼ºç‰ˆ"""
        try:
            # é¦–å…ˆè¿›è¡Œæ—‹è½¬æ£€æµ‹å’Œæ ¡æ­£
            corrected_img = self.detect_and_correct_rotation(img)
            
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(corrected_img, cv2.COLOR_BGR2GRAY)
            
            # å¤šç§é¢„å¤„ç†æ–¹æ¡ˆï¼Œæé«˜8ä½æ•°å­—è¯†åˆ«ç‡
            preprocessed_images = []
            
            # æ–¹æ¡ˆ1: æ ‡å‡†å¤„ç†
            denoised1 = cv2.GaussianBlur(gray, (3, 3), 0)
            clahe1 = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
            enhanced1 = clahe1.apply(denoised1)
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened1 = cv2.filter2D(enhanced1, -1, kernel)
            binary1 = cv2.adaptiveThreshold(sharpened1, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            preprocessed_images.append(binary1)
            
            # æ–¹æ¡ˆ2: å¼ºå¯¹æ¯”åº¦å¤„ç†ï¼ˆé’ˆå¯¹æ¨¡ç³Šçš„8ä½æ•°å­—ï¼‰
            denoised2 = cv2.GaussianBlur(gray, (5, 5), 0)
            clahe2 = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(6, 6))
            enhanced2 = clahe2.apply(denoised2)
            # æ›´å¼ºçš„é”åŒ–
            kernel_strong = np.array([[-2,-2,-2], [-2,17,-2], [-2,-2,-2]])
            sharpened2 = cv2.filter2D(enhanced2, -1, kernel_strong)
            binary2 = cv2.adaptiveThreshold(sharpened2, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, 3)
            preprocessed_images.append(binary2)
            
            # æ–¹æ¡ˆ3: ä¼½é©¬æ ¡æ­£ + åŒè¾¹æ»¤æ³¢
            gamma = 1.3
            lookup_table = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in np.arange(0, 256)]).astype("uint8")
            gamma_corrected = cv2.LUT(gray, lookup_table)
            bilateral = cv2.bilateralFilter(gamma_corrected, 9, 75, 75)
            binary3 = cv2.adaptiveThreshold(bilateral, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 13, 2)
            preprocessed_images.append(binary3)
            
            # æ–¹æ¡ˆ4: å¯¹æ¯”åº¦å¢å¼º
            enhanced4 = cv2.convertScaleAbs(gray, alpha=1.8, beta=40)
            denoised4 = cv2.GaussianBlur(enhanced4, (3, 3), 0)
            binary4 = cv2.adaptiveThreshold(denoised4, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 9, 2)
            preprocessed_images.append(binary4)
            
            # æ–¹æ¡ˆ5: å½¢æ€å­¦å¢å¼º
            denoised5 = cv2.GaussianBlur(gray, (3, 3), 0)
            clahe5 = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(7, 7))
            enhanced5 = clahe5.apply(denoised5)
            # å¼€è¿ç®—å»é™¤å™ªç‚¹
            kernel_open = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))
            opened = cv2.morphologyEx(enhanced5, cv2.MORPH_OPEN, kernel_open)
            binary5 = cv2.adaptiveThreshold(opened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
            preprocessed_images.append(binary5)
            
            # å¯¹æ¯ä¸ªé¢„å¤„ç†å›¾åƒè¿›è¡Œå½¢æ€å­¦æ“ä½œ
            final_images = []
            for binary in preprocessed_images:
                # å½¢æ€å­¦æ“ä½œï¼Œå»é™¤å™ªç‚¹
                kernel = np.ones((2, 2), np.uint8)
                cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
                cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel)
                
                # ç¡®ä¿å›¾åƒæ˜¯3é€šé“çš„ï¼Œå› ä¸ºPaddleOCRæœŸæœ›å½©è‰²å›¾åƒ
                if len(cleaned.shape) == 2:
                    cleaned = cv2.cvtColor(cleaned, cv2.COLOR_GRAY2BGR)
                
                final_images.append(cleaned)
            
            return final_images
            
        except Exception as e:
            logger.error(f"çŒªè€³æ ‡å›¾åƒé¢„å¤„ç†é”™è¯¯: {e}")
            return [img]
    
    def create_rotated_images(self, img, angles=[0, 90, 180, 270, 45, 135, 225, 315]):
        """åˆ›å»ºå¤šä¸ªæ—‹è½¬è§’åº¦çš„å›¾åƒç”¨äºè¯†åˆ«é¢ å€’çš„æ•°å­—"""
        rotated_images = []
        for angle in angles:
            if angle == 0:
                rotated_images.append(img)
            else:
                # è®¡ç®—æ—‹è½¬ä¸­å¿ƒ
                height, width = img.shape[:2]
                center = (width // 2, height // 2)
                
                # åˆ›å»ºæ—‹è½¬çŸ©é˜µ
                rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
                
                # æ‰§è¡Œæ—‹è½¬
                rotated = cv2.warpAffine(img, rotation_matrix, (width, height))
                rotated_images.append(rotated)
        
        return rotated_images
    
    def enhance_image_for_blur_detection(self, img):
        """ä¸“é—¨é’ˆå¯¹æ¨¡ç³Šå›¾åƒçš„å¢å¼ºå¤„ç†"""
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            if len(img.shape) == 3:
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            else:
                gray = img.copy()
            
            # 1. åº”ç”¨CLAHEå¢å¼ºå¯¹æ¯”åº¦ï¼ˆæ›´å¼ºï¼‰
            clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8,8))
            enhanced = clahe.apply(gray)
            
            # 2. é«˜æ–¯æ¨¡ç³Šå»å™ª
            denoised = cv2.GaussianBlur(enhanced, (3, 3), 0)
            
            # 3. é”åŒ–å¤„ç†
            kernel_sharpen = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(denoised, -1, kernel_sharpen)
            
            # 4. è‡ªé€‚åº”é˜ˆå€¼äºŒå€¼åŒ–ï¼ˆé’ˆå¯¹æ¨¡ç³Šå›¾åƒä¼˜åŒ–ï¼‰
            binary = cv2.adaptiveThreshold(sharpened, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 5)
            
            # 5. å½¢æ€å­¦æ“ä½œï¼šé—­è¿ç®—è¿æ¥æ–­å¼€çš„ç¬”ç”»
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            
            # 6. å½¢æ€å­¦æ“ä½œï¼šå¼€è¿ç®—å»é™¤å°å™ªç‚¹
            kernel_open = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
            cleaned = cv2.morphologyEx(closed, cv2.MORPH_OPEN, kernel_open)
            
            # ç¡®ä¿å›¾åƒæ˜¯3é€šé“çš„
            if len(cleaned.shape) == 2:
                cleaned = cv2.cvtColor(cleaned, cv2.COLOR_GRAY2BGR)
            
            return cleaned
            
        except Exception as e:
            logger.error(f"æ¨¡ç³Šå›¾åƒå¢å¼ºé”™è¯¯: {e}")
            return img
    
    def enhanced_ocr_image_for_eartag(self, image_bytes):
        """å¢å¼ºç‰ˆçŒªè€³æ ‡OCRè¯†åˆ« - æ™ºèƒ½åˆ†å±‚ç­–ç•¥"""
        try:
            # è§£ç å›¾åƒ
            nparr = np.frombuffer(image_bytes, np.uint8)
            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            
            if img is None:
                logger.error("âŒ æ— æ³•è§£ç å›¾åƒ")
                return []
            
            all_results = []
            
            # === ç¬¬ä¸€å±‚ï¼šå¿«é€Ÿè¯†åˆ«ï¼ˆåŸå›¾ + å¤šç§é¢„å¤„ç†ï¼‰===
            logger.info("ğŸ· ã€ç¬¬ä¸€å±‚ã€‘å¿«é€Ÿè¯†åˆ«...")
            
            # 1. åŸå›¾è¯†åˆ«
            result_original = self.ocr.ocr(img, cls=True)
            if result_original and result_original[0]:
                all_results.extend(result_original[0])
            
            # 2. å¤šç§é¢„å¤„ç†å›¾åƒè¯†åˆ«
            processed_imgs = self.preprocess_image_for_eartag(img)
            for processed_img in processed_imgs:
                result_processed = self.ocr.ocr(processed_img, cls=True)
                if result_processed and result_processed[0]:
                    all_results.extend(result_processed[0])
            
            # 3. å¢å¼ºé¢„å¤„ç†å›¾åƒè¯†åˆ«
            enhanced_img = self.enhance_image_for_blur_detection(img)
            result_enhanced = self.ocr.ocr(enhanced_img, cls=True)
            if result_enhanced and result_enhanced[0]:
                all_results.extend(result_enhanced[0])
            
            # æ£€æŸ¥ç¬¬ä¸€å±‚æ˜¯å¦æ£€æµ‹åˆ°è¶³å¤Ÿçš„è€³æ ‡å€™é€‰æ•°å­—
            eartag_candidates = 0
            for result in all_results:
                if result and len(result) >= 2:
                    text = result[1][0]
                    if self.is_valid_eartag_number(text):
                        eartag_candidates += 1
            
            logger.info(f"ğŸ· ç¬¬ä¸€å±‚è¯†åˆ«ç»“æœï¼šæ£€æµ‹åˆ° {eartag_candidates} ä¸ªè€³æ ‡å€™é€‰æ•°å­—")
            
            # === ç¬¬äºŒå±‚ï¼šå¤šè§’åº¦æ—‹è½¬å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰===
            if eartag_candidates < 2:
                logger.info("ğŸ· ã€ç¬¬äºŒå±‚ã€‘å¤šè§’åº¦æ—‹è½¬å¤„ç†...")
                
                # åˆ›å»ºå¤šè§’åº¦æ—‹è½¬å›¾åƒ
                rotated_images = self.create_rotated_images(img)
                
                for rotated_img in rotated_images:
                    result_rotated = self.ocr.ocr(rotated_img, cls=True)
                    if result_rotated and result_rotated[0]:
                        all_results.extend(result_rotated[0])
                
                logger.info("ğŸ· ç¬¬äºŒå±‚å¤šè§’åº¦æ—‹è½¬å¤„ç†å®Œæˆ")
            
            # åˆå¹¶å’Œå»é‡ç»“æœ
            unique_results = []
            seen_texts = set()
            
            for result in all_results:
                if result and len(result) >= 2:
                    text = result[1][0]
                    confidence = result[1][1]
                    
                    # æ¸…ç†æ–‡æœ¬ç”¨äºå»é‡
                    clean_text = ''.join(c for c in text if c.isalnum())
                    
                    if clean_text not in seen_texts:
                        unique_results.append({
                            "text": text,
                            "confidence": confidence,
                            "bbox": result[0]
                        })
                        seen_texts.add(clean_text)
            
            logger.info(f"âœ… çŒªè€³æ ‡æ™ºèƒ½åˆ†å±‚OCRè¯†åˆ«åˆ° {len(unique_results)} ä¸ªæ–‡æœ¬å—")
            return unique_results
            
        except Exception as e:
            logger.error(f"çŒªè€³æ ‡OCRè¯†åˆ«é”™è¯¯: {e}")
            return []
    
    def extract_pig_ear_tag_enhanced(self, texts_with_boxes):
        """æå–çŒªè€³æ ‡ä¸­çš„è€³æ ‡å·ç  - åŸºäºç§‘å­¦æ–¹æ¡ˆ"""
        # åˆå§‹åŒ–ç»“æœ
        result = {
            "ear_tag_7digit": "æœªè¯†åˆ«",  # 7ä½è€³æ ‡å·ç 
            "ear_tag_8digit": "æœªè¯†åˆ«",  # 8ä½è€³æ ‡å·ç 
        }
        
        # åˆ†ç±»å­˜å‚¨ç»“æœ
        eartag_numbers = []      # è€³æ ‡æ•°å­—
        other_numbers = []       # å…¶ä»–æ•°å­—
        text_content = []        # æ–‡æœ¬å†…å®¹
        
        # å¤„ç†æ‰€æœ‰è¯†åˆ«ç»“æœï¼Œè¿›è¡Œåˆ†ç±»
        for word_info in texts_with_boxes:
            try:
                text = word_info["text"]        # è¯†åˆ«çš„æ–‡å­—
                confidence = word_info.get("confidence", 0.0)  # ç½®ä¿¡åº¦
                
                # åˆ†ç±»å¤„ç†
                if self.is_valid_eartag_number(text):
                    eartag_numbers.append((text, confidence))
                elif any(c.isdigit() for c in text):
                    other_numbers.append((text, confidence))
                else:
                    text_content.append((text, confidence))
                    
            except:
                continue
        
        logger.info(f"ğŸ” è€³æ ‡æ•°å­—å€™é€‰: {[num[0] for num in eartag_numbers]}")
        logger.info(f"ğŸ” å…¶ä»–æ•°å­—å€™é€‰: {[num[0] for num in other_numbers]}")
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºè€³æ ‡æ•°å­—
        eartag_numbers.sort(key=lambda x: x[1], reverse=True)
        
        # å»é‡å¹¶æå–æœ€å¯èƒ½çš„ä¸¤ä¸ªæ•°å­—
        seen_numbers = set()
        valid_eartag_numbers = []
        
        for text, confidence in eartag_numbers:
            clean_text = ''.join(c for c in text if c.isalnum())
            if clean_text not in seen_numbers and len(clean_text) in [7, 8]:
                valid_eartag_numbers.append((clean_text, confidence))
                seen_numbers.add(clean_text)
        
        logger.info(f"ğŸ” æœ‰æ•ˆè€³æ ‡æ•°å­—: {valid_eartag_numbers}")
        
        # é€‰æ‹©æœ€å¯èƒ½çš„ä¸¤ä¸ªæ•°å­—
        if len(valid_eartag_numbers) >= 2:
            # æŒ‰ç½®ä¿¡åº¦é€‰æ‹©å‰ä¸¤ä¸ª
            first_num = valid_eartag_numbers[0][0]
            second_num = valid_eartag_numbers[1][0]
            
            # æ ¹æ®é•¿åº¦åˆ†é…7ä½å’Œ8ä½æ•°å­—
            if len(first_num) == 7 and len(second_num) == 8:
                result["ear_tag_7digit"] = first_num
                result["ear_tag_8digit"] = second_num
            elif len(first_num) == 8 and len(second_num) == 7:
                result["ear_tag_7digit"] = second_num
                result["ear_tag_8digit"] = first_num
            elif len(first_num) == 7 and len(second_num) == 7:
                # ä¸¤ä¸ªéƒ½æ˜¯7ä½ï¼Œé€‰æ‹©ç½®ä¿¡åº¦é«˜çš„ä½œä¸º7ä½ï¼Œå¦ä¸€ä¸ªè¡¥0ä½œä¸º8ä½
                result["ear_tag_7digit"] = first_num
                result["ear_tag_8digit"] = second_num + "0"
            elif len(first_num) == 8 and len(second_num) == 8:
                # ä¸¤ä¸ªéƒ½æ˜¯8ä½ï¼Œé€‰æ‹©ç½®ä¿¡åº¦é«˜çš„ä½œä¸º8ä½ï¼Œå¦ä¸€ä¸ªæˆªå–å‰7ä½
                result["ear_tag_7digit"] = second_num[:7]
                result["ear_tag_8digit"] = first_num
            else:
                # å…¶ä»–æƒ…å†µï¼ŒæŒ‰ç½®ä¿¡åº¦åˆ†é…
                result["ear_tag_7digit"] = first_num if len(first_num) == 7 else first_num[:7]
                result["ear_tag_8digit"] = second_num if len(second_num) == 8 else second_num + "0"
                
        elif len(valid_eartag_numbers) == 1:
            # åªæœ‰ä¸€ä¸ªæœ‰æ•ˆæ•°å­—
            num = valid_eartag_numbers[0][0]
            if len(num) == 7:
                result["ear_tag_7digit"] = num
                result["ear_tag_8digit"] = num + "0"
            elif len(num) == 8:
                result["ear_tag_7digit"] = num[:7]
                result["ear_tag_8digit"] = num
            else:
                result["ear_tag_7digit"] = num
                result["ear_tag_8digit"] = "æœªè¯†åˆ«"
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»å…¶ä»–æ•°å­—ä¸­å¯»æ‰¾
        if result["ear_tag_7digit"] == "æœªè¯†åˆ«" or result["ear_tag_8digit"] == "æœªè¯†åˆ«":
            other_valid_numbers = []
            for text, confidence in other_numbers:
                clean_text = ''.join(c for c in text if c.isalnum())
                if len(clean_text) in [7, 8] and clean_text not in seen_numbers:
                    other_valid_numbers.append((clean_text, confidence))
            
            other_valid_numbers.sort(key=lambda x: x[1], reverse=True)
            
            if other_valid_numbers:
                for num, confidence in other_valid_numbers:
                    if result["ear_tag_7digit"] == "æœªè¯†åˆ«" and len(num) == 7:
                        result["ear_tag_7digit"] = num
                    elif result["ear_tag_8digit"] == "æœªè¯†åˆ«" and len(num) == 8:
                        result["ear_tag_8digit"] = num
                    elif result["ear_tag_7digit"] == "æœªè¯†åˆ«" and len(num) == 8:
                        result["ear_tag_7digit"] = num[:7]
                    elif result["ear_tag_8digit"] == "æœªè¯†åˆ«" and len(num) == 7:
                        result["ear_tag_8digit"] = num + "0"
        
        logger.info(f"ğŸ“Œ ç§‘å­¦çŒªè€³æ ‡æå–ç»“æœ: {result}")
        return result
    
    def recognize_eartag(self, image_bytes):
        """çŒªè€³æ ‡è¯†åˆ«ä¸»å‡½æ•°"""
        try:
            # æ‰§è¡Œå¢å¼ºOCRè¯†åˆ«
            texts_with_boxes = self.enhanced_ocr_image_for_eartag(image_bytes)
            
            if not texts_with_boxes:
                logger.warning("âš ï¸ æœªè¯†åˆ«åˆ°ä»»ä½•æ–‡æœ¬")
                return {
                    "ear_tag_7digit": "æœªè¯†åˆ«",
                    "ear_tag_8digit": "æœªè¯†åˆ«"
                }
            
            # æå–è€³æ ‡æ•°å­—
            result = self.extract_pig_ear_tag_enhanced(texts_with_boxes)
            return result
            
        except Exception as e:
            logger.error(f"çŒªè€³æ ‡è¯†åˆ«é”™è¯¯: {e}")
            return {
                "ear_tag_7digit": "æœªè¯†åˆ«",
                "ear_tag_8digit": "æœªè¯†åˆ«"
            }

# åˆ›å»ºå…¨å±€å®ä¾‹
eartag_ocr = EartagOCR()

def recognize_pig_ear_tag(image_bytes):
    """çŒªè€³æ ‡è¯†åˆ«æ¥å£å‡½æ•°"""
    return eartag_ocr.recognize_eartag(image_bytes)
